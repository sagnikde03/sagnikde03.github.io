<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sagnik De's Homepage</title>
  
  <meta name="author" content="Sagnik De">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <div id="particles-js"></div>
  <div class="toggle-container">
    <input type="checkbox" id="dark-mode-toggle" onclick="toggleDarkMode()">
    <label for="dark-mode-toggle" class="toggle"></label>
  </div>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr style="padding:0px"> <td style="padding:0px"> <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr style="padding:0px"> <td style="padding:2.5%;width:63%;vertical-align:middle"> <p style="text-align:center"> <name>Sagnik De</name>
              </p>
              <p>
               I completed my <b>B.Tech</b> in <b>Electronics and Communication Engineering</b> in 2025, from <strong>Institute of Radio Physics and Electronics (IRPE)</strong>, <a href="https://www.caluniv.ac.in/">University of Calcutta</a>, India.<br>
              </p>
              <p>
    I am currently a Research Assistant at the <b>Cognitive Brain Dynamics Lab, School of AI & Data Science, IIT Jodhpur</b>, 
    under <a href="https://scholar.google.co.in/citations?user=9Z9gTzEAAAAJ&hl=en" target="_blank">Prof. Dipanjan Roy</a>, 
    where I work on computational models of brain resting-state dynamics and neuromodulation using tACS. 
    Previously, I was a Winter Research Intern at <b>IIT Delhi</b> with 
    <a href="https://scholar.google.com/citations?user=VyIA2PwAAAAJ&hl=en" target="_blank">Prof. Tapan Kumar Gandhi</a>, 
    focusing on deep learning optimization for multimodal anxiety detection from biopotential signals.
  </p>

  <p>
    I also served as a Research Intern at <b>MANIT Bhopal</b> under 
    <a href="https://scholar.google.com/citations?user=zZ3vDAsAAAAJ&hl=en" target="_blank">Dr. Varun Bajaj</a>, 
    contributing to EEG-based anxiety detection frameworks, and at <b>IIIT Naya Raipur</b> under 
    <a href="https://scholar.google.co.in/citations?user=bzoViqQAAAAJ&hl=en" target="_blank">Dr. Anurag Singh</a>, 
    where I developed multimodal deep learning methods for Major Depressive Disorder diagnosis. 
    Earlier, I worked at <b>CDAC Pune</b> with <a href="https://www.linkedin.com/in/akgupta5592" target="_blank">Dr. Anil Kumar Gupta</a> 
    on EEG-based early detection of Parkinson‚Äôs Disease and pathological brain state classification. 
    At the <b>University of Calcutta</b>, under 
    <a href="https://scholar.google.com/citations?user=pKsjvYMAAAAJ&hl=en" target="_blank">Dr. Anisha Halder Roy</a>, 
    I explored multimodal EEG‚ÄìsEMG fusion for pain assessment and brain activity analysis during olfactory and taste perception.
  </p>

                
              <p style="text-align:center">
               <a href="mailto:sagnikde2003@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV_Final (1).pdf">CV</a> &nbsp/&nbsp
		<a href="https://scholar.google.com/citations?user=foOv3C0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/sagnik-cu-a85127224/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/sagnikde03/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="#"><img style="width:100%;max-width:100%" alt="profile photo" src="data/SD_circle2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


		  <table align=center width=600px>
                <tr>
                  <td align=center width=600px>
                    <center><img src="data/logonp.png" height="95x" width="760x"></img><br></center>

                  </td>
                </tr>
        </table>

		  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My primary research interests span <strong>Artificial Intelligence & IoT in Healthcare</strong>, <strong>Biomedical Signal Processing</strong>, <strong>Human-Computer Interaction (HCI)</strong>, <strong>Computational Neuroscience</strong> 
		and <strong> Medical Image Analysis</strong> . I am deeply fascinated by the potential of these fields to shape the future of technology and transform the way we interact with machines and information. 
            <br>
                  <br>
                I am always open to new collaborations and research ideas. Feel free to reach out if you are interested in working together!
              </p>
            </td>
          </tr>
        </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Publications / Pre-Prints</heading>
        <p></p>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            
            <!-- GLEAM -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src="images/gleam.jpg" width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://doi.org/10.1016/j.compbiomed.2025.109928" target="_blank">
                  <papertitle>GLEAM: A Multimodal Deep Learning Framework for Chronic Lower Back Pain Detection Using EEG and sEMG Signals</papertitle>
                </a>
                <br>
                <strong>Sagnik De</strong>, Prithwijit Mukherjee, Anisha Halder Roy
                <br> <br>
                <a href="https://www.sciencedirect.com/journal/computers-in-biology-and-medicine" target="_blank">
                <em>Computers in Biology and Medicine</em>
                </a>, 2025
                <br> <br>
                <a href="javascript:void(0);" onclick="toggleAbstract('abs-gleam')">Abstract</a> /
                <a href="https://doi.org/10.1016/j.compbiomed.2025.109928">Article</a> 
                <p></p>
                <p id="abs-gleam" style="display:none;">
                  Low Back Pain (LBP) is the most prevalent musculoskeletal condition worldwide and a leading cause of disability, significantly affecting mobility, work productivity, and overall quality of life. Due to its high prevalence and substantial economic burden, LBP presents a critical global public health challenge that demands innovative diagnostic and therapeutic solutions. This study introduces a novel deep-learning approach for diagnosing LBP intensity using electroencephalography (EEG) signals and surface electromyography (sEMG) signals from back muscles. A GAN-Convolution-Transformer-based model, named GLEAM (GAN-ConvoLution-sElf Attention-ETLSTM), is designed to classify LBP intensity into four categories: no LBP, mild LBP, moderate LBP, and intolerable LBP. A denoising GAN is central to the model‚Äôs functionality, playing a pivotal role in enhancing the quality of EEG and sEMG signals by removing noise, resulting in cleaner and more accurate input data. Various features are extracted from the GAN-denoised EEG and sEMG signals, and the combined features from both EEG and sEMG are used for LBP detection. After the feature extraction, the CNN is employed to capture local temporal patterns within the data, allowing the model to focus on smaller, region-specific trends in the signals. Subsequently, the self-attention module identifies global correlations among these locally extracted features, enhancing the model‚Äôs ability to recognize broader patterns. The proposed ETLSTM network performs the final classification, which achieves an impressive LBP detection accuracy of 98.95%. This research presents several innovative contributions: (i) the development of a novel denoising GAN for cleaning EEG and sEMG signals, (ii) the design and integration of a new ETLSTM architecture as a classifier within the GLEAM model, and (iii) the introduction of the GLEAM hybrid deep learning framework, which enables robust and reliable LBP intensity assessment.
                </p>
              </td>
            </tr>

            <!-- TasteNet -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src="images/tastenet.jpg" width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://doi.org/10.1016/j.jneumeth.2025.110463" target="_blank">
                  <papertitle>TasteNet: A Novel Deep Learning Approach for EEG-Based Basic Taste Perception Recognition Using CEEMDAN Domain Entropy Features</papertitle>
                </a>
                <br>
                <strong>Sagnik De</strong>, Prithwijit Mukherjee, Anisha Halder Roy
                <br>  <br>
               
                <a href="https://www.sciencedirect.com/journal/journal-of-neuroscience-methods" target="_blank">
                <em>Journal of Neuroscience Methods</em>
                </a>, 2025
                <br>  <br>
                <a href="javascript:void(0);" onclick="toggleAbstract('abs-tastenet')">Abstract</a> /
                <a href="https://doi.org/10.1016/j.jneumeth.2025.110463">Article</a> 
                <p></p>
                <p id="abs-tastenet" style="display:none;">
                  Taste perception is the process by which the gustatory system detects and interprets chemical stimuli from food and beverages, involving activation of taste receptors on the tongue. Analyzing taste perception is essential for understanding human sensory responses and diagnosing taste-related disorders. This research focuses on developing a deep learning framework to effectively recognize basic taste stimuli from EEG signals. Initially, the recorded EEG signals undergo preprocessing to remove noise and artifacts. The CEEMDAN (complete ensemble empirical mode decomposition with adaptive noise) method is then applied to decompose the EEG signals into various frequency rhythms, referred to as intrinsic mode functions (IMFs). From the chosen IMFs, six distinct entropy features ‚Äî sample, bubble, approximate, dispersion, slope, and permutation entropy ‚Äî are extracted for further analysis. A novel deep learning model, TasteNet, is then developed, integrating a convolutional neural network (CNN) module, a multi-head attention module, and the Att-BiPLSTM (Attention-Bidirectional Potent Long Short-Term Memory) network. The proposed architecture classifies the input data into six categories: no taste, sweet, sour, bitter, umami, and salty, achieving a remarkable accuracy of 97.52 ¬± 0.48%. TasteNet outperforms existing taste perception classification methods, as demonstrated through extensive experiments. This study presents TasteNet, a robust framework for precise taste perception recognition using EEG signals. Using CEEMDAN for effective signal decomposition and extracting key entropy features, the model captures intricate patterns in taste stimuli. The incorporation of multi-head attention module and the Att-BiPLSTM network further enhances the model‚Äôs ability to identify various taste sensations accurately.
                </p>
              </td>
            </tr>

          </tbody>
        </table>

      </td>
    </tr>
  </tbody>
</table>

<script>
function toggleAbstract(id) {
  var x = document.getElementById(id);
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script>

	 <!-- TasteNet -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src="images/tastenet.jpg" width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://doi.org/10.1016/j.jneumeth.2025.110463" target="_blank">
                  <papertitle>TasteNet: A Novel Deep Learning Approach for EEG-Based Basic Taste Perception Recognition Using CEEMDAN Domain Entropy Features</papertitle>
                </a>
                <br>
                <strong>Sagnik De</strong>, Prithwijit Mukherjee, Anisha Halder Roy
                <br>  <br>
               
                <a href="https://www.sciencedirect.com/journal/journal-of-neuroscience-methods" target="_blank">
                <em>Journal of Neuroscience Methods</em>
                </a>, 2025
                <br>  <br>
                <a href="javascript:void(0);" onclick="toggleAbstract('abs-tastenet')">Abstract</a> /
                <a href="https://doi.org/10.1016/j.jneumeth.2025.110463">Article</a> 
                <p></p>
                <p id="abs-tastenet" style="display:none;">
                  Taste perception is the process by which the gustatory system detects and interprets chemical stimuli from food and beverages, involving activation of taste receptors on the tongue. Analyzing taste perception is essential for understanding human sensory responses and diagnosing taste-related disorders. This research focuses on developing a deep learning framework to effectively recognize basic taste stimuli from EEG signals. Initially, the recorded EEG signals undergo preprocessing to remove noise and artifacts. The CEEMDAN (complete ensemble empirical mode decomposition with adaptive noise) method is then applied to decompose the EEG signals into various frequency rhythms, referred to as intrinsic mode functions (IMFs). From the chosen IMFs, six distinct entropy features ‚Äî sample, bubble, approximate, dispersion, slope, and permutation entropy ‚Äî are extracted for further analysis. A novel deep learning model, TasteNet, is then developed, integrating a convolutional neural network (CNN) module, a multi-head attention module, and the Att-BiPLSTM (Attention-Bidirectional Potent Long Short-Term Memory) network. The proposed architecture classifies the input data into six categories: no taste, sweet, sour, bitter, umami, and salty, achieving a remarkable accuracy of 97.52 ¬± 0.48%. TasteNet outperforms existing taste perception classification methods, as demonstrated through extensive experiments. This study presents TasteNet, a robust framework for precise taste perception recognition using EEG signals. Using CEEMDAN for effective signal decomposition and extracting key entropy features, the model captures intricate patterns in taste stimuli. The incorporation of multi-head attention module and the Att-BiPLSTM network further enhances the model‚Äôs ability to identify various taste sensations accurately.
                </p>
              </td>
            </tr>

          </tbody>
        </table>

      </td>
    </tr>
  </tbody>
</table>

<script>
function toggleAbstract(id) {
  var x = document.getElementById(id);
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script>
	




	
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>

          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="5"><tbody>
					
          <tr>
            <!--<td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>-->
            <td width="100%" valign="center">
              1. Won the 3rd Runners Up in <a href="https://sagnikde03.github.io/data/TELECAST_2024.pdf" target="_blank">TELECAST 2024</a> organized by
				University of Calcutta, Kolkata in collaboration with CTiF, India <br>
			  2. Won the 1st Prize in <a href="https://sagnikde03.github.io/data/Cognitech.pdf" target="_blank"> COGNITECH 2023</a> organized by 
				AI & Robotics Club in collaboration with IEEE Calcutta University Student Branch <br>
			  3. Won the 1st Prize in <a href="https://sagnikde03.github.io/data/Astronova_Cert.pdf" target="_blank"> Research Work Presentation 2023</a> organized by 
				IEEE Photonics Society Kolkata Chapter, IEEE APS Kolkata Chapter & IEEE Calcutta University Student Branch <br>
              4. Reviewer: IEEE Access, Biomedical Signal Processing & Control, Food Chemistry, Scientific Reports, Biological Psychology.
            </td>
          </tr>
					
<!--
          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Basically <br> Blog Posts</heading>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
              <br>
              <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
              <br>
              <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
            </td>
          </tr>



			
-->		 

			
			
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                &copy Sagnik De (2025) <b>|</b> A person who never made a mistake never tried anything <a href="https://jonbarron.info/">new</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>


  <script src="particles.js-master/particles.js"></script>
<script>
  particlesJS("particles-js", {
    particles: {
      number: {
        value: 80,
        density: {
          enable: true,
          value_area: 800
        }
      },
      color: {
        value: "#005eff"
      },
      shape: {
        type: "circle",
        stroke: {
          width: 0,
          color: "#000000"
        },
        polygon: {
          nb_sides: 5
        }
      },
      opacity: {
        value: 0.3,
        random: false,
        anim: {
          enable: false,
          speed: 1,
          opacity_min: 0.1,
          sync: false
        }
      },
      size: {
        value: 3,
        random: true,
        anim: {
          enable: false,
          speed: 40,
          size_min: 0.1,
          sync: false
        }
      },
      line_linked: {
        enable: true,
        distance: 150,
        color: "#005eff",
        opacity: 0.4,
        width: 1
      },
      move: {
        enable: true,
        speed: 6,
        direction: "none",
        random: false,
        straight: false,
        out_mode: "out",
        attract: {
          enable: false,
          rotateX: 600,
          rotateY: 1200
        }
      }
    },
    interactivity: {
      detect_on: "canvas",
      events: {
        onhover: {
          enable: true,
          mode: "grab"
        },
        onclick: {
          enable: true,
          mode: "push"
        },
        resize: true
      },
      modes: {
        grab: {
          distance: 140,
          line_linked: {
            opacity: 1
          }
        },
        bubble: {
          distance: 400,
          size: 40,
          duration: 2,
          opacity: 8,
          speed: 3
        },
        repulse: {
          distance: 200,
          duration: 0.4
        },
        push: {
          particles_nb: 4
        },
        remove: {
          particles_nb: 2
        }
      }
    },
    retina_detect: true
  });
</script>
<script src="script.js"></script>
</body>

</html>
